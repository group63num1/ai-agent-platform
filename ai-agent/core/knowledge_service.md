"""
çŸ¥è¯†åº“æœåŠ¡ - å¤„ç†æ–‡æ¡£ä¸Šä¼ ã€chunkingã€å‘é‡åŒ–å’Œå­˜å‚¨
"""

import os
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime
import json

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import FastEmbedEmbeddings

from core.milvus_client import MilvusClient
from core.database import get_session, KnowledgeBaseDB, get_knowledge_base

logger = logging.getLogger(__name__)


class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # å»¶è¿ŸåŠ è½½ embedding æ¨¡å‹ï¼ˆåªåœ¨éœ€è¦æ—¶åŠ è½½ï¼‰
        self.embeddings = None
        self.document_root = Path("knowledge/origin_document")
        self.document_root.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… çŸ¥è¯†åº“æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def _ensure_embeddings_loaded(self):
        """ç¡®ä¿ embedding æ¨¡å‹å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self.embeddings is not None:
            return

        logger.info("â³ æ­£åœ¨åŠ è½½ embedding æ¨¡å‹ (FastEmbed - è½»é‡ç‰ˆ)...")
        try:
            # è½»é‡çº§ ONNX æ¨¡å‹ï¼Œé¿å…å®‰è£… torch
            self.embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
            logger.info("âœ… å·²åŠ è½½ FastEmbed æ¨¡å‹: BAAI/bge-small-zh-v1.5")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ embedding æ¨¡å‹å¤±è´¥: {e}")
            raise

    def generate_kb_id(self, user_id: str, kb_name: str) -> str:
        """
        ç”ŸæˆçŸ¥è¯†åº“ ID

        Args:
            user_id: ç”¨æˆ· ID
            kb_name: çŸ¥è¯†åº“åç§°

        Returns:
            çŸ¥è¯†åº“ ID
        """
        # ä½¿ç”¨ user_id + kb_name çš„ç»„åˆç”Ÿæˆå”¯ä¸€ ID
        raw_id = f"{user_id}_{kb_name}"
        kb_id = hashlib.md5(raw_id.encode()).hexdigest()[:16]
        return f"{user_id}_{kb_id}"

    def save_uploaded_files(
        self,
        kb_id: str,
        files: List[Dict[str, Any]],
    ) -> List[str]:
        """
        ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°

        Args:
            kb_id: çŸ¥è¯†åº“ ID
            files: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å« filename å’Œ content

        Returns:
            ä¿å­˜åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        kb_dir = self.document_root / kb_id
        kb_dir.mkdir(parents=True, exist_ok=True)

        saved_paths = []

        for file_info in files:
            filename = file_info.get("filename")
            content = file_info.get("content")

            if not filename or content is None:
                logger.warning(f"âš ï¸  æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡")
                continue

            # ä¿å­˜æ–‡ä»¶
            file_path = kb_dir / filename
            try:
                # å¦‚æœ content æ˜¯å­—èŠ‚æµï¼Œç›´æ¥å†™å…¥ï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ŒæŒ‰æ–‡æœ¬å†™å…¥
                if isinstance(content, bytes):
                    with open(file_path, "wb") as f:
                        f.write(content)
                else:
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(content)

                saved_paths.append(str(file_path))
                logger.info(f"âœ… å·²ä¿å­˜æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")

        return saved_paths

    def load_document_content(self, file_path: str) -> str:
        """
        åŠ è½½æ–‡æ¡£å†…å®¹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡æ¡£å†…å®¹
        """
        file_path = Path(file_path)

        if not file_path.exists():
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return ""

        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½å†…å®¹
            suffix = file_path.suffix.lower()

            if suffix == ".txt":
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()

            elif suffix == ".md":
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()

            elif suffix == ".pdf":
                # TODO: å®ç° PDF è§£æ
                logger.warning(f"âš ï¸  æš‚ä¸æ”¯æŒ PDF æ ¼å¼: {file_path}")
                return ""

            else:
                logger.warning(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
                return ""

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

    def chunk_documents(
        self,
        file_paths: List[str],
        chunking_method: str = "recursive",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ) -> List[Dict[str, Any]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œ chunking

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            chunking_method: chunking æ–¹å¼
            chunk_size: chunk å¤§å°
            chunk_overlap: chunk é‡å 

        Returns:
            chunk åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« text, source, chunk_idx
        """
        # æ ¹æ® chunking_method é€‰æ‹©åˆ†å‰²å™¨
        # æ”¯æŒçš„æ–¹å¼: recursive, fixed, sentence, token, markdown
        if chunking_method == "recursive":
            # é€’å½’å­—ç¬¦åˆ†å‰²ï¼ˆæ¨èï¼Œæ™ºèƒ½å¤„ç†æ®µè½ã€å¥å­ï¼‰
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
            )
        elif chunking_method == "fixed":
            # å›ºå®šå­—ç¬¦åˆ†å‰²ï¼ˆæŒ‰æ¢è¡Œç¬¦ï¼‰
            from langchain_text_splitters import CharacterTextSplitter

            splitter = CharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separator="\n",
            )
        elif chunking_method == "sentence":
            # æŒ‰å¥å­åˆ†å‰²ï¼ˆé€‚åˆå¯¹è¯ã€é—®ç­”ï¼‰
            from langchain_text_splitters import SentenceTransformersTokenTextSplitter

            splitter = SentenceTransformersTokenTextSplitter(
                chunk_overlap=chunk_overlap,
                tokens_per_chunk=chunk_size // 4,  # è½¬æ¢ä¸º token æ•°
            )
        elif chunking_method == "token":
            # æŒ‰ token åˆ†å‰²ï¼ˆç²¾ç¡®æ§åˆ¶ token æ•°é‡ï¼‰
            from langchain_text_splitters import TokenTextSplitter

            splitter = TokenTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
            )
        elif chunking_method == "markdown":
            # Markdown æ–‡æ¡£ä¸“ç”¨åˆ†å‰²ï¼ˆä¿ç•™ç»“æ„ï¼‰
            from langchain_text_splitters import MarkdownTextSplitter

            splitter = MarkdownTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
            )
        else:
            # é»˜è®¤ä½¿ç”¨ recursive
            logger.warning(
                f"âš ï¸  æœªçŸ¥çš„ chunking æ–¹å¼: {chunking_method}ï¼Œä½¿ç”¨é»˜è®¤ recursive"
            )
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
            )

        all_chunks = []

        for file_path in file_paths:
            content = self.load_document_content(file_path)
            if not content:
                continue

            try:
                chunks = splitter.split_text(content)
                for chunk_idx, chunk in enumerate(chunks):
                    all_chunks.append(
                        {
                            "text": chunk,
                            "source": str(Path(file_path).name),
                            "file_path": str(file_path),
                            "chunk_idx": chunk_idx,
                        }
                    )
                logger.info(f"âœ… {Path(file_path).name} åˆ†å‰²ä¸º {len(chunks)} ä¸ª chunks")
            except Exception as e:
                logger.error(f"âŒ åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return all_chunks

    def create_knowledge_base(
        self,
        user_id: str,
        name: str,
        files: List[Dict[str, Any]],
        description: str = "",
        chunking_method: str = "recursive",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        enabled: bool = True,
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºçŸ¥è¯†åº“

        Args:
            user_id: ç”¨æˆ· ID
            name: çŸ¥è¯†åº“åç§°
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            description: çŸ¥è¯†åº“æè¿°
            chunking_method: chunking æ–¹å¼
            chunk_size: chunk å¤§å°
            chunk_overlap: chunk é‡å 
            enabled: æ˜¯å¦å¯ç”¨

        Returns:
            åˆ›å»ºç»“æœ
        """
        try:
            # 1. ç”ŸæˆçŸ¥è¯†åº“ ID
            kb_id = self.generate_kb_id(user_id, name)
            logger.info(f"í ½í³ åˆ›å»ºçŸ¥è¯†åº“: {kb_id}")

            # 2. ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_paths = self.save_uploaded_files(kb_id, files)
            if not file_paths:
                return {"success": False, "error": "æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•æ–‡ä»¶"}

            # 3. å¯¹æ–‡æ¡£è¿›è¡Œ chunking
            chunks = self.chunk_documents(
                file_paths,
                chunking_method=chunking_method,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
            )

            if not chunks:
                return {
                    "success": False,
                    "error": "æ–‡æ¡£ chunking å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½• chunk",
                }

            logger.info(f"âœ… æ€»å…±ç”Ÿæˆ {len(chunks)} ä¸ª chunks")

            # 4. ç”Ÿæˆ embeddings
            self._ensure_embeddings_loaded()
            texts = [chunk["text"] for chunk in chunks]
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"âœ… å·²ç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")

            # 5. åˆ›å»º Milvus é›†åˆå¹¶å­˜å‚¨å‘é‡
            milvus_client = MilvusClient(collection_name=kb_id)

            # åˆ›å»ºé›†åˆ
            milvus_client.create_collection_if_not_exists(
                collection_name=kb_id,
                vector_dim=len(embeddings[0]),
                similarity_metric="IP",
            )

            # æ’å…¥å‘é‡
            sources = [chunk["source"] for chunk in chunks]
            ids = milvus_client.insert_vectors(
                texts=texts,
                embeddings=embeddings,
                sources=sources,
                batch_size=100,
            )

            logger.info(f"âœ… å·²å­˜å‚¨ {len(ids)} ä¸ªå‘é‡åˆ° Milvus é›†åˆ: {kb_id}")

            # 6. ä¿å­˜çŸ¥è¯†åº“ä¿¡æ¯åˆ°æ•°æ®åº“
            with get_session() as session:
                kb = KnowledgeBaseDB(
                    kb_id=kb_id,
                    user_id=user_id,
                    name=name,
                    description=description,
                    file_paths=file_paths,
                    chunking_method=chunking_method,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    total_chunks=len(chunks),
                    milvus_collection=kb_id,
                    enabled=enabled,
                    created_at=datetime.now().isoformat(),
                    updated_at=datetime.now().isoformat(),
                )
                session.add(kb)
                session.commit()
                logger.info(f"âœ… çŸ¥è¯†åº“ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")

            milvus_client.close()

            return {
                "success": True,
                "kb_id": kb_id,
                "name": name,
                "total_files": len(file_paths),
                "total_chunks": len(chunks),
                "milvus_collection": kb_id,
            }

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def query_knowledge_base(
        self,
        kb_id: str,
        query: str,
        top_k: int = 5,
        similarity_threshold: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“

        Args:
            kb_id: çŸ¥è¯†åº“ ID
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            self._ensure_embeddings_loaded()
            query_embedding = self.embeddings.embed_query(query)

            # ä» Milvus æœç´¢
            milvus_client = MilvusClient(collection_name=kb_id)
            results = milvus_client.search(query_embedding, top_k=top_k * 2)

            # è¿‡æ»¤ç›¸ä¼¼åº¦
            filtered_results = []
            for result in results:
                similarity = min(result["distance"], 1.0)
                if similarity >= similarity_threshold:
                    filtered_results.append({**result, "similarity": similarity})

            milvus_client.close()

            # æ’åºå¹¶é™åˆ¶æ•°é‡
            filtered_results.sort(key=lambda x: x["similarity"], reverse=True)
            return filtered_results[:top_k]

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return []

    def search_similar_content(
        self,
        query_text: str,
        kb_id: str = None,
        user_id: str = None,
        limit: int = 5,
        similarity_threshold: float = 0.0,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼å†…å®¹
        - kb_id: æŒ‡å®šçŸ¥è¯†åº“æœç´¢
        - user_id: ç”¨æˆ·IDï¼ˆè‹¥kb_idä¸ºç©ºï¼Œåˆ™è·¨è¯¥ç”¨æˆ·æ‰€æœ‰çŸ¥è¯†åº“æœç´¢ï¼‰
        """
        try:
            from core.database import list_knowledge_bases, get_knowledge_base
            from core.milvus_retriever import MilvusRetriever

            # 1. ç¡®å®šæœç´¢çš„çŸ¥è¯†åº“
            if kb_id:
                # æŒ‡å®šçŸ¥è¯†åº“æœç´¢
                kb_info = get_knowledge_base(kb_id)
                if not kb_info:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}")
                    return []
                kb_ids = [kb_id]
                kb_name_map = {kb_id: kb_info["name"]}
            elif user_id:
                # ç”¨æˆ·çš„æ‰€æœ‰çŸ¥è¯†åº“
                kbs = list_knowledge_bases(enabled_only=True)
                kbs = [kb for kb in kbs if kb.get("user_id") == user_id]
                if not kbs:
                    logger.warning(f"ç”¨æˆ· {user_id} æ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“")
                    return []
                kb_ids = [kb["kb_id"] for kb in kbs]
                kb_name_map = {kb["kb_id"]: kb["name"] for kb in kbs}
            else:
                logger.warning("å¿…é¡»æŒ‡å®š kb_id æˆ– user_id")
                return []

            logger.info(
                f"í ½í´ åœ¨ {len(kb_ids)} ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢å‰ {limit} æ¡: {query_text[:50]}..."
            )

            retriever = MilvusRetriever()

            # åªå–å‰ top_kï¼ˆé»˜è®¤ 5ï¼‰
            results = retriever.retrieve(
                query_text=query_text,
                kb_ids=kb_ids,
                k=limit,
                score_threshold=0.0,
            )

            # æ·»åŠ çŸ¥è¯†åº“åç§°
            for result in results:
                result_kb_id = result.get("kb_id")
                result["kb_name"] = kb_name_map.get(result_kb_id, "æœªçŸ¥çŸ¥è¯†åº“")

            # é˜ˆå€¼è¿‡æ»¤ï¼ˆä¿ç•™æ»¡è¶³é˜ˆå€¼çš„å…¨éƒ¨ï¼Œæœ€å¤š limit æ¡ï¼‰
            filtered = [
                r
                for r in results
                if r.get("similarity_score", 0) >= similarity_threshold
            ]
            logger.info(
                f"âœ… é˜ˆå€¼ {similarity_threshold}, åŸå§‹ {len(results)} æ¡ï¼Œè¿‡æ»¤å {len(filtered)} æ¡"
            )

            return filtered[:limit]

        except Exception as e:
            logger.error(f"âŒ æœç´¢ç›¸ä¼¼å†…å®¹å¤±è´¥: {e}", exc_info=True)
            return []

    def get_top_chunks(
        self, kb_id: str = None, user_id: str = None, limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        æ—  query æ—¶è¿”å›å‰ N æ¡ chunk
        - kb_id: æŒ‡å®šçŸ¥è¯†åº“ï¼ˆè¿”å›è¯¥çŸ¥è¯†åº“çš„å‰ N æ¡ï¼‰
        - user_id: ç”¨æˆ·IDï¼ˆè‹¥kb_idä¸ºç©ºï¼Œè¿”å›ç”¨æˆ·æ‰€æœ‰çŸ¥è¯†åº“çš„å‰ N æ¡ï¼‰
        """
        try:
            from core.database import list_knowledge_bases, get_knowledge_base

            # 1. ç¡®å®šæŸ¥è¯¢çš„çŸ¥è¯†åº“
            if kb_id:
                # æŒ‡å®šçŸ¥è¯†åº“
                kb_info = get_knowledge_base(kb_id)
                if not kb_info:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}")
                    return []
                kbs = [kb_info]
            elif user_id:
                # ç”¨æˆ·çš„æ‰€æœ‰çŸ¥è¯†åº“
                kbs = list_knowledge_bases(enabled_only=True)
                kbs = [kb for kb in kbs if kb.get("user_id") == user_id]
                if not kbs:
                    logger.warning(f"ç”¨æˆ· {user_id} æ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“")
                    return []
            else:
                logger.warning("å¿…é¡»æŒ‡å®š kb_id æˆ– user_id")
                return []

            remaining = limit
            results: List[Dict[str, Any]] = []

            for kb in kbs:
                if remaining <= 0:
                    break

                take = remaining
                milvus_client = MilvusClient(collection_name=kb["kb_id"])
                try:
                    top_chunks = milvus_client.query_top(limit=take)
                    for item in top_chunks:
                        item["kb_id"] = kb["kb_id"]
                        item["kb_name"] = kb["name"]
                    results.extend(top_chunks)
                    remaining = max(0, remaining - len(top_chunks))
                finally:
                    milvus_client.close()

            logger.info(f"âœ… æ—  queryï¼Œè¿”å› {len(results)} æ¡ chunk (limit={limit})")
            return results[:limit]

        except Exception as e:
            logger.error(f"âŒ è·å– top chunk å¤±è´¥: {e}", exc_info=True)
            return []

    def rebuild_knowledge_base(
        self,
        kb_id: str,
        files: Optional[List[Dict[str, Any]]] = None,
        chunking_method: Optional[str] = None,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        name: Optional[str] = None,
        description: Optional[str] = None,
        enabled: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """
        é‡æ„çŸ¥è¯†åº“å‘é‡

        å½“æ–‡ä»¶å†…å®¹ã€chunking æ–¹å¼æˆ–å‚æ•°å‘ç”Ÿå˜åŒ–æ—¶ï¼Œéœ€è¦é‡æ–°å‘é‡åŒ–

        Args:
            kb_id: çŸ¥è¯†åº“ ID
            files: æ–°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™æ›¿æ¢æ—§æ–‡ä»¶ï¼‰
            chunking_method: æ–°çš„ chunking æ–¹å¼
            chunk_size: æ–°çš„ chunk å¤§å°
            chunk_overlap: æ–°çš„ chunk é‡å 
            name: æ–°çš„çŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰
            description: æ–°çš„æè¿°ï¼ˆå¯é€‰ï¼‰
            enabled: æ–°çš„å¯ç”¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰

        Returns:
            é‡æ„ç»“æœ
        """
        try:
            from core.database import get_knowledge_base, update_knowledge_base

            # 1. è·å–ç°æœ‰çŸ¥è¯†åº“ä¿¡æ¯
            kb = get_knowledge_base(kb_id)

            if not kb:
                return {"success": False, "error": f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}"}

            logger.info(f"í ½í´„ å¼€å§‹é‡æ„çŸ¥è¯†åº“: {kb_id}")

            # 2. ç¡®å®šæ–°çš„å‚æ•°ï¼ˆå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨æ—§å€¼ï¼‰
            new_chunking_method = chunking_method or kb.chunking_method
            new_chunk_size = chunk_size or kb.chunk_size
            new_chunk_overlap = chunk_overlap or kb.chunk_overlap

            # 3. å¤„ç†æ–‡ä»¶
            if files is not None:
                # æœ‰æ–°æ–‡ä»¶ï¼šåˆ é™¤æ—§æ–‡ä»¶ï¼Œä¿å­˜æ–°æ–‡ä»¶
                old_dir = self.document_root / kb_id
                if old_dir.exists():
                    import shutil

                    shutil.rmtree(old_dir)
                    logger.info(f"í ½í·‘ï¸  å·²åˆ é™¤æ—§æ–‡ä»¶ç›®å½•")

                file_paths = self.save_uploaded_files(kb_id, files)
                if not file_paths:
                    return {"success": False, "error": "æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•æ–‡ä»¶"}
            else:
                # æ— æ–°æ–‡ä»¶ï¼šä½¿ç”¨æ—§æ–‡ä»¶è·¯å¾„
                file_paths = (
                    kb.file_paths
                    if isinstance(kb.file_paths, list)
                    else json.loads(kb.file_paths or "[]")
                )

            # 4. é‡æ–° chunk æ–‡æ¡£
            chunks = self.chunk_documents(
                file_paths,
                chunking_method=new_chunking_method,
                chunk_size=new_chunk_size,
                chunk_overlap=new_chunk_overlap,
            )

            if not chunks:
                return {"success": False, "error": "æ–‡æ¡£ chunking å¤±è´¥"}

            logger.info(f"âœ… é‡æ–°ç”Ÿæˆ {len(chunks)} ä¸ª chunks")

            # 5. ç”Ÿæˆæ–°çš„ embeddings
            self._ensure_embeddings_loaded()
            texts = [chunk["text"] for chunk in chunks]
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"âœ… å·²ç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")

            # 6. åˆ é™¤æ—§çš„ Milvus é›†åˆ
            old_milvus_collection = kb.milvus_collection
            milvus_client = MilvusClient(collection_name=old_milvus_collection)
            try:
                milvus_client.drop_collection()
                logger.info(f"í ½í·‘ï¸  å·²åˆ é™¤æ—§çš„ Milvus é›†åˆ: {old_milvus_collection}")
            except Exception as e:
                logger.warning(f"âš ï¸  åˆ é™¤æ—§é›†åˆå¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")

            # 7. åˆ›å»ºæ–°çš„ Milvus é›†åˆå¹¶å­˜å‚¨å‘é‡
            milvus_client.create_collection_if_not_exists(
                collection_name=kb_id,
                vector_dim=len(embeddings[0]),
                similarity_metric="IP",
            )

            sources = [chunk["source"] for chunk in chunks]
            ids = milvus_client.insert_vectors(
                texts=texts,
                embeddings=embeddings,
                sources=sources,
                batch_size=100,
            )

            logger.info(f"âœ… å·²å­˜å‚¨ {len(ids)} ä¸ªå‘é‡åˆ°æ–°é›†åˆ: {kb_id}")
            milvus_client.close()

            # 8. æ›´æ–°æ•°æ®åº“è®°å½•
            updates = {
                "file_paths": file_paths,
                "chunking_method": new_chunking_method,
                "chunk_size": new_chunk_size,
                "chunk_overlap": new_chunk_overlap,
                "total_chunks": len(chunks),
                "milvus_collection": kb_id,
                "updated_at": datetime.now().isoformat(),
            }

            # æ·»åŠ å¯é€‰æ›´æ–°å­—æ®µ
            if name is not None:
                updates["name"] = name
            if description is not None:
                updates["description"] = description
            if enabled is not None:
                updates["enabled"] = enabled

            with get_session() as session:
                update_knowledge_base(session, kb_id, **updates)

            logger.info(f"âœ… çŸ¥è¯†åº“é‡æ„å®Œæˆ: {kb_id}")

            return {
                "success": True,
                "kb_id": kb_id,
                "total_files": len(file_paths),
                "total_chunks": len(chunks),
                "message": "çŸ¥è¯†åº“é‡æ„æˆåŠŸ",
            }

        except Exception as e:
            logger.error(f"âŒ é‡æ„çŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def delete_knowledge_base(self, kb_id: str) -> Dict[str, Any]:
        """
        åˆ é™¤çŸ¥è¯†åº“

        åŒæ—¶åˆ é™¤ Milvus é›†åˆã€æ–‡ä»¶ç›®å½•å’Œæ•°æ®åº“è®°å½•

        Args:
            kb_id: çŸ¥è¯†åº“ ID

        Returns:
            åˆ é™¤ç»“æœ
        """
        try:
            from core.database import delete_knowledge_base as db_delete_kb

            logger.info(f"{'='*60}")
            logger.info(f"í ¾í·ª å¼€å§‹æ‰§è¡Œåˆ é™¤çŸ¥è¯†åº“æ“ä½œ")
            logger.info(f"{'='*60}")
            logger.info(f"   kb_id: {kb_id}")

            logger.info(f"í ½í³ ç¬¬ 1 æ­¥: ä»æ•°æ®åº“æŸ¥è¯¢çŸ¥è¯†åº“ä¿¡æ¯")

            # 1. ä»æ•°æ®åº“è·å–çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆåœ¨ session å†…è·å–æ‰€æœ‰å±æ€§ï¼‰
            with get_session() as session:
                logger.info(f"   - å·²æ‰“å¼€æ•°æ®åº“ session")
                kb = (
                    session.query(KnowledgeBaseDB)
                    .filter(KnowledgeBaseDB.kb_id == kb_id)
                    .first()
                )
                logger.info(f"   - æŸ¥è¯¢å®Œæˆ, kb å¯¹è±¡: {kb}")

                if not kb:
                    logger.warning(f"   âš ï¸  çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}")
                    return {"success": False, "error": f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_id}"}

                # åœ¨ session å†…è·å–æ‰€æœ‰éœ€è¦çš„å±æ€§å€¼
                milvus_collection = kb.milvus_collection
                logger.info(f"   âœ… è·å–æˆåŠŸ:")
                logger.info(f"      - kb_id: {kb.kb_id}")
                logger.info(f"      - name: {kb.name}")
                logger.info(f"      - milvus_collection: {milvus_collection}")
                logger.info(f"      - total_chunks: {kb.total_chunks}")

            logger.info(f"í ½í³ ç¬¬ 2 æ­¥: åˆ é™¤ Milvus é›†åˆ")

            # 2. åˆ é™¤ Milvus é›†åˆ
            milvus_client = MilvusClient(collection_name=milvus_collection)
            try:
                logger.info(f"   - å‡†å¤‡åˆ é™¤é›†åˆ: {milvus_collection}")
                milvus_client.delete_collection()
                logger.info(f"   âœ… å·²åˆ é™¤ Milvus é›†åˆ: {milvus_collection}")
            except Exception as e:
                logger.warning(f"   âš ï¸  åˆ é™¤ Milvus é›†åˆå¤±è´¥: {e}")
            finally:
                milvus_client.close()
                logger.info(f"   - Milvus å®¢æˆ·ç«¯å·²å…³é—­")

            logger.info(f"í ½í³ ç¬¬ 3 æ­¥: åˆ é™¤æ–‡ä»¶ç›®å½•")

            # 3. åˆ é™¤æ–‡ä»¶ç›®å½•
            kb_dir = self.document_root / kb_id
            if kb_dir.exists():
                import shutil

                logger.info(f"   - ç›®å½•å­˜åœ¨: {kb_dir}")
                shutil.rmtree(kb_dir)
                logger.info(f"   âœ… å·²åˆ é™¤æ–‡ä»¶ç›®å½•: {kb_dir}")
            else:
                logger.info(f"   âš ï¸  æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {kb_dir}")

            logger.info(f"í ½í³ ç¬¬ 4 æ­¥: åˆ é™¤æ•°æ®åº“è®°å½•")

            # 4. åˆ é™¤æ•°æ®åº“è®°å½•
            logger.info(f"   - è°ƒç”¨æ•°æ®åº“åˆ é™¤å‡½æ•°")
            ok = db_delete_kb(kb_id)
            logger.info(f"   - åˆ é™¤ç»“æœ: {ok}")

            if not ok:
                logger.warning(f"   âš ï¸  åˆ é™¤æ•°æ®åº“è®°å½•å¤±è´¥æˆ–è®°å½•ä¸å­˜åœ¨: {kb_id}")
            else:
                logger.info(f"   âœ… æ•°æ®åº“è®°å½•å·²åˆ é™¤")

            logger.info(f"{'='*60}")
            logger.info(f"âœ… çŸ¥è¯†åº“åˆ é™¤å®Œæˆ: {kb_id}")
            logger.info(f"{'='*60}")

            return {
                "success": True,
                "message": "çŸ¥è¯†åº“åˆ é™¤æˆåŠŸ",
            }

        except Exception as e:
            logger.error(f"{'='*60}")
            logger.error(f"âŒ åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
            logger.error(f"{'='*60}")
            return {"success": False, "error": str(e)}


# å…¨å±€æœåŠ¡å®ä¾‹
_kb_service = None


def get_kb_service() -> KnowledgeBaseService:
    """è·å–çŸ¥è¯†åº“æœåŠ¡å•ä¾‹"""
    global _kb_service
    if _kb_service is None:
        _kb_service = KnowledgeBaseService()
    return _kb_service

